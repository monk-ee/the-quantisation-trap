name: Quantization Hallucination Trap Experiment

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'eval_hallucination.py'
      - 'hallucination_report.py'
      - 'hallucination_quant.jsonl'
  workflow_dispatch:
    inputs:
      experiment_type:
        description: 'Experiment type'
        required: true
        type: choice
        options:
          - simple_quantization
          - hyperparameter_sweep
        default: 'hyperparameter_sweep'
      model_name:
        description: 'Hugging Face model name'
        required: false
        default: 'meta-llama/Meta-Llama-3-8B-Instruct'
      max_tokens:
        description: 'Maximum tokens to generate per response'
        required: false
        default: '80'
      test_subset:
        description: 'Test only subset of quantization types (comma-separated: none,int8,nf4,fp4_double_quant)'
        required: false
        default: 'none,int8,nf4,fp4_double_quant'
      prompt_engineering:
        description: 'Test prompt engineering effects'
        required: false
        type: choice
        options:
          - both
          - baseline_only
          - prompted_only
        default: 'both'

jobs:
  # Single job running all quantization experiments sequentially 
  # Model stays loaded in GPU memory across experiments
  quantization-hallucination-experiment:
    runs-on:
      - machine
      - gpu=l40s
      - cpu=8
      - ram=64
      - architecture=x64
      - tenancy=spot
    timeout-minutes: 180
    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_HUB_ENABLE_HF_TRANSFER: 1
      HF_HUB_DOWNLOAD_TIMEOUT: 120
      CUDA_LAUNCH_BLOCKING: 1
      TORCH_USE_CUDA_DSA: 1
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install uv
      uses: astral-sh/setup-uv@v4

    - name: Install dependencies
      run: |
        echo "🚀 Installing ML dependencies for quantization experiments..."
        uv pip install -r requirements-hf.txt --system
        
        # Verify critical dependencies for quantization
        echo "🧪 Verifying quantization dependencies..."
        python -c "import torch; print('✅ PyTorch:', torch.__version__, '| CUDA available:', torch.cuda.is_available())"
        python -c "import transformers; print('✅ transformers:', transformers.__version__)"
        python -c "import bitsandbytes; print('✅ bitsandbytes available for quantization')" || {
          echo "💥 bitsandbytes missing - required for quantization experiments!"
          exit 1
        }
        
        # GPU verification
        python -c "
        import torch
        if torch.cuda.is_available():
            print(f'✅ GPU: {torch.cuda.get_device_name(0)}')
            print(f'✅ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
            print(f'✅ CUDA Version: {torch.version.cuda}')
        else:
            print('⚠️ No GPU available')
        "

    - name: Login to Hugging Face
      run: |
        python -c "from huggingface_hub import login; login('${{ secrets.HF_TOKEN }}')"

    - name: Create output directories
      run: |
        mkdir -p hallucination_results hyperparam_results
        echo "📁 Created output directories"
        echo "🧪 Starting Quantization Hallucination Experiment..."
        echo "📋 Experiment Type: ${{ github.event.inputs.experiment_type || 'hyperparameter_sweep' }}"
        echo "📋 Prompt Engineering: ${{ github.event.inputs.prompt_engineering || 'both' }}"

    - name: Parse quantization types
      id: parse_quant
      run: |
        # Parse comma-separated quantization types
        QUANT_TYPES="${{ github.event.inputs.test_subset || 'none,int8,nf4,fp4,nf4_double_quant,fp4_double_quant' }}"
        echo "quantization_types=$QUANT_TYPES" >> $GITHUB_OUTPUT
        echo "🎯 Testing quantization types: $QUANT_TYPES"

    # SIMPLE QUANTIZATION EXPERIMENTS (deterministic sampling)
    - name: "Simple Quantization - No Quantization (Baseline)"
      if: contains(steps.parse_quant.outputs.quantization_types, 'none') && github.event.inputs.experiment_type == 'simple_quantization'
      run: |
        echo "🔬 SIMPLE BASELINE EXPERIMENT - No Quantization"
        echo "============================================="
        
        python eval_hallucination.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant.jsonl \
          --quantization none \
          --output hallucination_results/results_none.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    # HYPERPARAMETER SWEEP EXPERIMENTS (multiple sampling configs)
    - name: "Hyperparam Sweep - No Quantization (Baseline Prompts)"
      if: contains(steps.parse_quant.outputs.quantization_types, 'none') && (github.event.inputs.experiment_type == 'hyperparameter_sweep' || github.event.inputs.experiment_type == '') && (github.event.inputs.prompt_engineering == 'both' || github.event.inputs.prompt_engineering == 'baseline_only' || github.event.inputs.prompt_engineering == '')
      run: |
        echo "🔬 HYPERPARAM SWEEP - No Quantization (Baseline Prompts)"
        echo "======================================================="
        
        python eval_hallucination_hyperparams.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant.jsonl \
          --quantization none \
          --output hyperparam_results/hyperparam_none_baseline.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    - name: "Hyperparam Sweep - No Quantization (Anti-Hallucination Prompts)"
      if: contains(steps.parse_quant.outputs.quantization_types, 'none') && (github.event.inputs.experiment_type == 'hyperparameter_sweep' || github.event.inputs.experiment_type == '') && (github.event.inputs.prompt_engineering == 'both' || github.event.inputs.prompt_engineering == 'prompted_only')
      run: |
        echo "🔬 HYPERPARAM SWEEP - No Quantization (Anti-Hallucination Prompts)"
        echo "=================================================================="
        echo "🧠 Testing: Does prompt engineering reduce hallucination more than quantization?"
        
        python eval_hallucination_hyperparams.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant_prompted.jsonl \
          --quantization none \
          --output hyperparam_results/hyperparam_none_prompted.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    - name: "Simple Quantization - INT8"
      if: contains(steps.parse_quant.outputs.quantization_types, 'int8') && github.event.inputs.experiment_type == 'simple_quantization'
      run: |
        echo "🔬 SIMPLE INT8 QUANTIZATION EXPERIMENT"
        echo "====================================="
        
        python eval_hallucination.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant.jsonl \
          --quantization int8 \
          --output hallucination_results/results_int8.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    - name: "Hyperparam Sweep - INT8 (Baseline Prompts)"
      if: contains(steps.parse_quant.outputs.quantization_types, 'int8') && (github.event.inputs.experiment_type == 'hyperparameter_sweep' || github.event.inputs.experiment_type == '') && (github.event.inputs.prompt_engineering == 'both' || github.event.inputs.prompt_engineering == 'baseline_only' || github.event.inputs.prompt_engineering == '')
      run: |
        echo "🔬 HYPERPARAM SWEEP - INT8 (Baseline Prompts)"
        echo "============================================="
        
        python eval_hallucination_hyperparams.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant.jsonl \
          --quantization int8 \
          --output hyperparam_results/hyperparam_int8_baseline.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    - name: "Hyperparam Sweep - INT8 (Anti-Hallucination Prompts)"
      if: contains(steps.parse_quant.outputs.quantization_types, 'int8') && (github.event.inputs.experiment_type == 'hyperparameter_sweep' || github.event.inputs.experiment_type == '') && (github.event.inputs.prompt_engineering == 'both' || github.event.inputs.prompt_engineering == 'prompted_only')
      run: |
        echo "🔬 HYPERPARAM SWEEP - INT8 (Anti-Hallucination Prompts)"
        echo "======================================================"
        
        python eval_hallucination_hyperparams.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant_prompted.jsonl \
          --quantization int8 \
          --output hyperparam_results/hyperparam_int8_prompted.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    - name: "Simple Quantization - NF4"
      if: contains(steps.parse_quant.outputs.quantization_types, 'nf4') && github.event.inputs.experiment_type == 'simple_quantization'
      run: |
        echo "🔬 SIMPLE NF4 QUANTIZATION EXPERIMENT"
        echo "===================================="
        
        python eval_hallucination.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant.jsonl \
          --quantization nf4 \
          --output hallucination_results/results_nf4.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    - name: "Hyperparam Sweep - NF4 (Baseline Prompts)"
      if: contains(steps.parse_quant.outputs.quantization_types, 'nf4') && (github.event.inputs.experiment_type == 'hyperparameter_sweep' || github.event.inputs.experiment_type == '') && (github.event.inputs.prompt_engineering == 'both' || github.event.inputs.prompt_engineering == 'baseline_only' || github.event.inputs.prompt_engineering == '')
      run: |
        echo "🔬 HYPERPARAM SWEEP - NF4 (Baseline Prompts)"
        echo "============================================"
        
        python eval_hallucination_hyperparams.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant.jsonl \
          --quantization nf4 \
          --output hyperparam_results/hyperparam_nf4_baseline.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    - name: "Hyperparam Sweep - NF4 (Anti-Hallucination Prompts)"
      if: contains(steps.parse_quant.outputs.quantization_types, 'nf4') && (github.event.inputs.experiment_type == 'hyperparameter_sweep' || github.event.inputs.experiment_type == '') && (github.event.inputs.prompt_engineering == 'both' || github.event.inputs.prompt_engineering == 'prompted_only')
      run: |
        echo "🔬 HYPERPARAM SWEEP - NF4 (Anti-Hallucination Prompts)"
        echo "====================================================="
        
        python eval_hallucination_hyperparams.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant_prompted.jsonl \
          --quantization nf4 \
          --output hyperparam_results/hyperparam_nf4_prompted.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    - name: "Simple Quantization - FP4 Double (NUCLEAR)"
      if: contains(steps.parse_quant.outputs.quantization_types, 'fp4_double_quant') && github.event.inputs.experiment_type == 'simple_quantization'
      run: |
        echo "💥 SIMPLE FP4 DOUBLE QUANTIZATION (NUCLEAR)"
        echo "=========================================="
        
        python eval_hallucination.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant.jsonl \
          --quantization fp4_double_quant \
          --output hallucination_results/results_fp4_double_quant.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    - name: "Hyperparam Sweep - FP4 Double (NUCLEAR - Baseline Prompts)"
      if: contains(steps.parse_quant.outputs.quantization_types, 'fp4_double_quant') && (github.event.inputs.experiment_type == 'hyperparameter_sweep' || github.event.inputs.experiment_type == '') && (github.event.inputs.prompt_engineering == 'both' || github.event.inputs.prompt_engineering == 'baseline_only' || github.event.inputs.prompt_engineering == '')
      run: |
        echo "💥 HYPERPARAM SWEEP - FP4 DOUBLE (NUCLEAR - Baseline Prompts)"
        echo "============================================================="
        echo "🚨 Testing hyperparameter sensitivity under extreme quantization"
        
        python eval_hallucination_hyperparams.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant.jsonl \
          --quantization fp4_double_quant \
          --output hyperparam_results/hyperparam_fp4_double_quant_baseline.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    - name: "Hyperparam Sweep - FP4 Double (NUCLEAR - Anti-Hallucination Prompts)"
      if: contains(steps.parse_quant.outputs.quantization_types, 'fp4_double_quant') && (github.event.inputs.experiment_type == 'hyperparameter_sweep' || github.event.inputs.experiment_type == '') && (github.event.inputs.prompt_engineering == 'both' || github.event.inputs.prompt_engineering == 'prompted_only')
      run: |
        echo "💥 HYPERPARAM SWEEP - FP4 DOUBLE (NUCLEAR - Anti-Hallucination Prompts)"
        echo "======================================================================="
        echo "🧠 KEY EXPERIMENT: Can prompt engineering overcome extreme quantization effects?"
        
        python eval_hallucination_hyperparams.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --prompts hallucination_quant_prompted.jsonl \
          --quantization fp4_double_quant \
          --output hyperparam_results/hyperparam_fp4_double_quant_prompted.json \
          --max-tokens ${{ github.event.inputs.max_tokens || '80' }} \
          --verbose

    - name: Generate Simple Quantization Analysis
      if: github.event.inputs.experiment_type == 'simple_quantization'
      run: |
        echo "📊 GENERATING SIMPLE QUANTIZATION ANALYSIS"
        echo "=========================================="
        
        # Find all simple result files
        RESULT_FILES=""
        for quant_type in $(echo "${{ steps.parse_quant.outputs.quantization_types }}" | tr ',' ' '); do
          if [ -f "hallucination_results/results_${quant_type}.json" ]; then
            RESULT_FILES="$RESULT_FILES hallucination_results/results_${quant_type}.json"
          fi
        done
        
        echo "📁 Simple result files: $RESULT_FILES"
        
        if [ -n "$RESULT_FILES" ]; then
          python hallucination_report.py \
            --results $RESULT_FILES \
            --output hallucination_results/analysis_report.json \
            --verbose
        else
          echo "⚠️ No simple result files found"
        fi

    - name: Generate Hyperparameter Analysis with Graphs
      if: github.event.inputs.experiment_type == 'hyperparameter_sweep' || github.event.inputs.experiment_type == ''
      run: |
        echo "📊 GENERATING HYPERPARAMETER ANALYSIS WITH GRAPHS"
        echo "=================================================="
        
        # Find all hyperparam result files (baseline and prompted)
        RESULT_FILES=""
        for quant_type in $(echo "${{ steps.parse_quant.outputs.quantization_types }}" | tr ',' ' '); do
          if [ -f "hyperparam_results/hyperparam_${quant_type}_baseline.json" ]; then
            RESULT_FILES="$RESULT_FILES hyperparam_results/hyperparam_${quant_type}_baseline.json"
          fi
          if [ -f "hyperparam_results/hyperparam_${quant_type}_prompted.json" ]; then
            RESULT_FILES="$RESULT_FILES hyperparam_results/hyperparam_${quant_type}_prompted.json"
          fi
          # Legacy support for old naming
          if [ -f "hyperparam_results/hyperparam_${quant_type}.json" ]; then
            RESULT_FILES="$RESULT_FILES hyperparam_results/hyperparam_${quant_type}.json"
          fi
        done
        
        echo "📁 Hyperparam result files: $RESULT_FILES"
        
        if [ -n "$RESULT_FILES" ]; then
          python hyperparam_analysis_report.py \
            --results $RESULT_FILES \
            --output hyperparam_results/hyperparam_analysis.json \
            --verbose
            
          # Also run specialized prompt vs quantization analysis if both types exist
          echo ""
          echo "🧠 GENERATING PROMPT vs QUANTIZATION COMPARISON"
          echo "==============================================="
          python prompt_vs_quantization_analysis.py \
            --results $RESULT_FILES \
            --output hyperparam_results/prompt_vs_quantization_analysis.json \
            --verbose
        else
          echo "⚠️ No hyperparam result files found"
        fi

    - name: Display Simple Quantization Results
      if: github.event.inputs.experiment_type == 'simple_quantization'
      run: |
        echo "🎯 ============================================"
        echo "   SIMPLE QUANTIZATION EXPERIMENT RESULTS"
        echo "============================================="
        
        if [ -f "hallucination_results/analysis_report.json" ]; then
          echo "📊 SIMPLE QUANTIZATION SUMMARY:"
          echo "==============================="
          python -c "
        import json
        with open('hallucination_results/analysis_report.json', 'r') as f:
            report = json.load(f)
        
        summary = report['experiment_summary']
        print(f'Model: {summary[\"model_name\"]}')
        print(f'Quantization Types: {\", \".join(summary[\"quantization_types\"])}')
        print(f'Questions per Type: {summary[\"total_questions_per_type\"]}')
        print()
        
        # Display summary table
        if 'summary_table' in report:
            print('📈 RESULTS TABLE:')
            for row in report['summary_table'][:6]:  # Limit rows for readability
                print('  '.join(f'{str(cell):<12}' for cell in row))
            print()
        
        # Display conclusions
        if 'conclusions' in report:
            print('🔍 KEY FINDINGS:')
            for i, finding in enumerate(report['conclusions'], 1):
                print(f'{i}. {finding}')
          "
        fi
        
        echo ""
        echo "📁 Generated Files:"
        find hallucination_results/ -name "*.json" -type f -exec basename {} \;

    - name: Display Hyperparameter Sweep Results  
      if: github.event.inputs.experiment_type == 'hyperparameter_sweep' || github.event.inputs.experiment_type == ''
      run: |
        echo "🎯 ============================================"
        echo "   HYPERPARAMETER × QUANTIZATION SWEEP RESULTS"
        echo "============================================="
        
        if [ -f "hyperparam_results/hyperparam_analysis.json" ]; then
          echo "📊 HYPERPARAMETER SENSITIVITY SUMMARY:"
          echo "====================================="
          python -c "
        import json
        with open('hyperparam_results/hyperparam_analysis.json', 'r') as f:
            report = json.load(f)
        
        print(f'Model: {report[\"experiment_summary\"][\"model_name\"]}')
        print(f'Question Categories: {report[\"experiment_summary\"][\"question_categories\"]}')
        print(f'Hyperparameter Configs: {report[\"experiment_summary\"][\"total_hyperparam_configs\"]}')
        print(f'Experiment Types: {report[\"experiment_summary\"][\"quantization_types\"]}')
        print(f'Total Responses Analyzed: {report[\"experiment_summary\"][\"total_responses\"]}')
        print()
        
        # Show baseline vs prompted comparison if both exist
        baseline_experiments = [exp for exp in report[\"experiment_summary\"][\"quantization_types\"] if \"_baseline\" in exp]
        prompted_experiments = [exp for exp in report[\"experiment_summary\"][\"quantization_types\"] if \"_prompted\" in exp]
        
        if baseline_experiments and prompted_experiments:
            print('🧠 PROMPT ENGINEERING COMPARISON:')
            print('=================================')
            print(f'Baseline experiments: {len(baseline_experiments)}')
            print(f'Prompted experiments: {len(prompted_experiments)}')
            print('Testing: Does prompt engineering beat quantization for reducing hallucination?')
            print()
        
        # Show key findings
        if 'key_findings' in report:
            print('🔍 KEY DISCOVERIES:')
            for i, finding in enumerate(report['key_findings'], 1):
                print(f'{i}. {finding}')
            print()
        
        # Show sensitivity rankings
        if 'sensitivity_analysis' in report:
            sens = report['sensitivity_analysis']
            print('🌡️ TEMPERATURE SENSITIVITY RANKING:')
            for rank in sens.get('temperature_sensitivity_ranking', []):
                print(f'{rank[\"rank\"]}. {rank[\"quantization\"]} - Score: {rank[\"sensitivity_score\"]:.3f}')
            print()
        
        # Show generated visualizations
        if 'visualization_files' in report:
            print('📈 GENERATED VISUALIZATIONS:')
            for viz_file in report['visualization_files']:
                import os
                filename = os.path.basename(viz_file)
                if os.path.exists(viz_file):
                    print(f'✅ {filename}')
                else:
                    print(f'❌ {filename} (not found)')
            print()
          "
          
          # Show prompt vs quantization analysis if available
          if [ -f "hyperparam_results/prompt_vs_quantization_analysis.json" ]; then
            echo "🧠 PROMPT ENGINEERING vs QUANTIZATION RESULTS:"
            echo "=============================================="
            python -c "
        import json
        with open('hyperparam_results/prompt_vs_quantization_analysis.json', 'r') as f:
            prompt_analysis = json.load(f)
        
        summary = prompt_analysis['summary']
        analysis = prompt_analysis['analysis']
        
        print(f'Quantization Types Compared: {summary[\"total_comparisons\"]}')
        print(f'Prompt Engineering Wins: {summary[\"prompt_engineering_winner\"]}')
        print(f'Quantization Wins: {summary[\"quantization_winner\"]}')
        print()
        
        if 'prompt_engineering_effectiveness' in analysis:
            effectiveness = analysis['prompt_engineering_effectiveness']
            refusal_improvement = effectiveness['overall_refusal_improvement']
            hallucination_reduction = effectiveness['overall_hallucination_reduction']
            print(f'🎯 Overall Refusal Rate Improvement: {refusal_improvement:+.1%}')
            print(f'🎯 Overall Hallucination Rate Reduction: {hallucination_reduction:+.1%}')
            print()
        
        if analysis['key_findings']:
            print('💡 KEY INSIGHTS:')
            for finding in analysis['key_findings']:
                print(f'• {finding}')
            print()
            "
          fi
        fi
        
        echo ""
        echo "📁 Generated Files:"
        echo "JSON Results:"
        find hyperparam_results/ -name "*.json" -type f -exec basename {} \;
        echo "📈 Generated Graphs:"
        find hyperparam_results/ -name "*.png" -type f -exec basename {} \;

    - name: Upload Experiment Results
      uses: actions/upload-artifact@v4
      with:
        name: quantization-hallucination-results
        path: |
          hallucination_results/
          hyperparam_results/
          hallucination_quant.jsonl
          hallucination_quant_prompted.jsonl
        retention-days: 30

    - name: Create Experiment Summary
      run: |
        echo "🎯 ============================================" > experiment_summary.txt
        echo "   QUANTIZATION HALLUCINATION EXPERIMENT" >> experiment_summary.txt
        echo "   Experiment Type: ${{ github.event.inputs.experiment_type || 'hyperparameter_sweep' }}" >> experiment_summary.txt  
        echo "=============================================" >> experiment_summary.txt
        echo "" >> experiment_summary.txt
        
        EXPERIMENT_TYPE="${{ github.event.inputs.experiment_type || 'hyperparameter_sweep' }}"
        
        if [ "$EXPERIMENT_TYPE" = "simple_quantization" ] && [ -f "hallucination_results/analysis_report.json" ]; then
          echo "📊 SIMPLE QUANTIZATION RESULTS:" >> experiment_summary.txt
          echo "===============================" >> experiment_summary.txt
          python -c "
        import json
        with open('hallucination_results/analysis_report.json', 'r') as f:
            report = json.load(f)
        
        with open('experiment_summary.txt', 'a') as f:
            summary = report['experiment_summary']
            f.write(f'Model: {summary[\"model_name\"]}\n')
            f.write(f'Quantization Types: {\", \".join(summary[\"quantization_types\"])}\n')
            f.write(f'Questions per Type: {summary[\"total_questions_per_type\"]}\n\n')
            
            if 'conclusions' in report:
                f.write('KEY FINDINGS:\n')
                for i, finding in enumerate(report['conclusions'], 1):
                    f.write(f'{i}. {finding}\n')
                f.write('\n')
          "
        elif [ "$EXPERIMENT_TYPE" != "simple_quantization" ] && [ -f "hyperparam_results/hyperparam_analysis.json" ]; then
          echo "📊 HYPERPARAMETER SWEEP RESULTS:" >> experiment_summary.txt
          echo "================================" >> experiment_summary.txt
          python -c "
        import json
        with open('hyperparam_results/hyperparam_analysis.json', 'r') as f:
            report = json.load(f)
        
        with open('experiment_summary.txt', 'a') as f:
            summary = report['experiment_summary']
            f.write(f'Model: {summary[\"model_name\"]}\n')
            f.write(f'Question Categories: {summary[\"question_categories\"]}\n')
            f.write(f'Hyperparameter Configs: {summary[\"total_hyperparam_configs\"]}\n')
            f.write(f'Quantization Types: {summary[\"quantization_types\"]}\n')
            f.write(f'Total Responses: {summary[\"total_responses\"]}\n\n')
            
            if 'key_findings' in report:
                f.write('KEY DISCOVERIES:\n')
                for i, finding in enumerate(report['key_findings'], 1):
                    f.write(f'{i}. {finding}\n')
                f.write('\n')
                
            if 'sensitivity_analysis' in report:
                f.write('TEMPERATURE SENSITIVITY RANKING:\n')
                sens = report['sensitivity_analysis']
                for rank in sens.get('temperature_sensitivity_ranking', []):
                    f.write(f'{rank[\"rank\"]}. {rank[\"quantization\"]} - Score: {rank[\"sensitivity_score\"]:.3f}\n')
                f.write('\n')
                
            if 'visualization_files' in report:
                f.write(f'GENERATED VISUALIZATIONS: {len(report[\"visualization_files\"])}\n')
                for viz_file in report['visualization_files']:
                    import os
                    filename = os.path.basename(viz_file)
                    f.write(f'• {filename}\n')
                f.write('\n')
          "
        else
          echo "⚠️ No analysis results available" >> experiment_summary.txt
        fi
        
        echo "📁 BLOG POST POTENTIAL:" >> experiment_summary.txt
        echo "======================" >> experiment_summary.txt
        if [ "$EXPERIMENT_TYPE" = "simple_quantization" ]; then
          echo "Title: 'The Quantization Trap That Wasn't'" >> experiment_summary.txt
        else
          echo "Title: 'When Quantized Models Stop Listening: The Temperature Sensitivity Trap'" >> experiment_summary.txt
        fi
        echo "Download artifacts for detailed analysis and visualization" >> experiment_summary.txt
        
        cat experiment_summary.txt