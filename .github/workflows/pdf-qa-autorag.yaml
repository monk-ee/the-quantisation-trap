name: PDF Q&A AutoRAG Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'pdfs/**'
      - 'qa_extraction_lib/**'
      - 'cli_pdf_qa.py'
      - 'domain_eval_gpu.py'
  workflow_dispatch:
    inputs:
      input_file:
        description: 'Input PDF file path'
        required: true
        default: 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf'
      model_name:
        description: 'Hugging Face model name'
        required: false
        default: 'meta-llama/Meta-Llama-3-8B-Instruct'
      chunk_size:
        description: 'Chunk size in words'
        required: false
        default: '800'
      batch_size:
        description: 'Batch size for processing'
        required: false
        default: '8'
      use_quantization:
        description: 'Use 4-bit quantization to save GPU memory'
        required: false
        default: 'false'
      max_questions:
        description: 'Maximum number of evaluation questions'
        required: false
        default: '15'
      enable_bert_score:
        description: 'Enable BERT-score evaluation'
        required: false
        default: 'true'
      top_k_selection:
        description: 'Top K Q&A pairs to select for AutoRAG evaluation'
        required: false
        default: '50'

jobs:
  # Serial PDF Q&A Generation + AutoRAG Pipeline (All permutations in one job)
  pdf-qa-autorag-serial:
    runs-on:
      - machine
      - gpu=l40s
      - cpu=8
      - ram=64
      - architecture=x64
      - tenancy=spot
    timeout-minutes: 360
    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_HUB_ENABLE_HF_TRANSFER: 1
      HF_HUB_DOWNLOAD_TIMEOUT: 120
      CUDA_LAUNCH_BLOCKING: 1
      TORCH_USE_CUDA_DSA: 1
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install uv
      uses: astral-sh/setup-uv@v4

    - name: Install dependencies (Working Approach)
      run: |
        # Use the working approach from other pipelines
        echo "üöÄ Installing from working requirements files..."
        uv pip install -r requirements-hf.txt --system
        uv pip install -r requirements-domain-eval.txt --system
        
        # Verify FAISS-GPU installation worked
        echo "üß™ Verifying FAISS-GPU installation..."
        python -c "import faiss; assert hasattr(faiss, 'StandardGpuResources'), 'StandardGpuResources missing'; print('‚úÖ FAISS-GPU with GPU classes verified')" || {
          echo "üí• FAISS-GPU missing GPU classes - CRASHING PIPELINE!"
          exit 1
        }
        
        # Verify critical imports and GPU detection
        python -c "import sentence_transformers; print('‚úÖ sentence-transformers:', sentence_transformers.__version__)"
        python -c "import torch; print('‚úÖ torch:', torch.__version__, '| CUDA available:', torch.cuda.is_available(), '| Device count:', torch.cuda.device_count())"
        python -c "import transformers; print('‚úÖ transformers:', transformers.__version__)"
        python -c "import bitsandbytes; print('‚úÖ bitsandbytes available for quantization')"
        python -c "import faiss; print('‚úÖ faiss installed'); print('üîç FAISS GPU detection:', faiss.get_num_gpus() if hasattr(faiss, 'get_num_gpus') else 'N/A', 'GPUs available')"
        
    
    - name: Login to Hugging Face
      run: |
        python -c "from huggingface_hub import login; login('${{ secrets.HF_TOKEN }}')"

    - name: Create output directories
      run: |
        mkdir -p outputs rag_input rag_store autorag_results
        echo "üìÅ Created pipeline directories"
        echo "üöÄ AutoRAG Pipeline Starting..."

    # Run all 9 permutations sequentially (model loads once and stays in memory)
    - name: "Q&A Generation - Basic √ó High Creativity"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_basic_high_creativity_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.9 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels basic \
          --run-name "_basic_high_creativity" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Basic √ó Balanced"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_basic_balanced_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.7 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels basic \
          --run-name "_basic_balanced" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Basic √ó Conservative"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_basic_conservative_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 256 \
          --temperature 0.3 \
          --top-p 0.8 \
          --do-sample \
          --difficulty-levels basic \
          --run-name "_basic_conservative" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Intermediate √ó High Creativity"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_intermediate_high_creativity_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.9 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels intermediate \
          --run-name "_intermediate_high_creativity" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Intermediate √ó Balanced"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_intermediate_balanced_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.7 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels intermediate \
          --run-name "_intermediate_balanced" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Intermediate √ó Conservative"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_intermediate_conservative_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 256 \
          --temperature 0.3 \
          --top-p 0.8 \
          --do-sample \
          --difficulty-levels intermediate \
          --run-name "_intermediate_conservative" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Advanced √ó High Creativity"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_advanced_high_creativity_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.9 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels advanced \
          --run-name "_advanced_high_creativity" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Advanced √ó Balanced"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_advanced_balanced_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.7 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels advanced \
          --run-name "_advanced_balanced" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Advanced √ó Conservative"
      run: |
        python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_advanced_conservative_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 256 \
          --temperature 0.3 \
          --top-p 0.8 \
          --do-sample \
          --difficulty-levels advanced \
          --run-name "_advanced_conservative" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: Q&A Generation Summary
      run: |
        echo "üéØ ============================================"
        echo "   Q&A GENERATION COMPLETED - ALL PERMUTATIONS"
        echo "=============================================="
        
        echo "üìä Generated Q&A Files:"
        find outputs/ -name "*.jsonl" -type f -exec basename {} \; | sort
        
        echo ""
        echo "üìà Q&A Pairs Summary:"
        find outputs/ -name "*.jsonl" -type f -exec wc -l {} + | tail -1
        
        echo ""
        echo "üöÄ Starting AutoRAG pipeline with all permutation results..."

    # Continue with AutoRAG pipeline using all generated results
    - name: Create AutoRAG Input Directory
      run: |
        echo "üìÅ Preparing AutoRAG input from all permutations"
        
        # List all generated Q&A files
        echo "üì• Q&A Generation Results:"
        find outputs/ -name "*.jsonl" -type f | head -20
        
        # Count total Q&A pairs across all matrix combinations
        echo "üìä Q&A Pairs Summary:"
        find outputs/ -name "*.jsonl" -type f -exec wc -l {} + | tail -1
    
    - name: Select Top K Q&A Pairs
      run: |
        python qa_pair_selector.py \
          --qa-artifacts-dir outputs \
          --output-dir rag_input \
          --top-k ${{ github.event.inputs.top_k_selection || '50' }} \
          --verbose
    
    - name: Build Q&A Vector Store (Standard RAG)
      run: |
        python qa_faiss_builder.py \
          --qa-pairs-file rag_input/selected_qa_pairs.json \
          --output-dir rag_store \
          --embedding-model all-MiniLM-L6-v2 \
          --verbose
    
    - name: Run Standard RAG Evaluation
      run: |
        echo "üîπ STANDARD RAG EVALUATION"
        echo "=========================="
        
        python qa_autorag_evaluator.py \
          --qa-pairs-file rag_input/selected_qa_pairs.json \
          --qa-faiss-index rag_store/qa_faiss_index.bin \
          --qa-metadata rag_store/qa_metadata.json \
          --output-dir autorag_results/standard_rag \
          --model-name "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --embedding-model all-MiniLM-L6-v2 \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantization' || '' }} \
          --verbose
    
    - name: Run Base Model Evaluation (No RAG)
      run: |
        echo "üîπ BASE MODEL EVALUATION (NO RAG)"
        echo "=================================="
        echo "Testing model without any retrieval context"
        
        python qa_base_model_evaluator.py \
          --qa-pairs-file rag_input/selected_qa_pairs.json \
          --output-dir autorag_results/base_model \
          --model-name "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantization' || '' }} \
          --verbose
    
    - name: Compare Standard RAG vs Base Model Performance
      run: |
        echo "üìä RAG vs BASE MODEL COMPARISON"
        echo "==============================="
        
        python rag_vs_base_comparison.py \
          --standard-results autorag_results/standard_rag \
          --base-results autorag_results/base_model \
          --output-file autorag_results/rag_vs_base_comparison.json \
          --verbose
    
    - name: RAG Demonstration with Amplifier Questions
      run: |
        echo "üé∏ RAG DEMONSTRATION: AMPLIFIER Q&A"
        echo "===================================="
        echo "Demonstrating RAG capabilities with real amplifier questions"
        
        python rag_demo_questions.py \
          --qa-faiss-index rag_store/qa_faiss_index.bin \
          --qa-metadata rag_store/qa_metadata.json \
          --model-name "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --embedding-model all-MiniLM-L6-v2 \
          --output-file autorag_results/rag_demonstration.json \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantization' || '' }} \
          --verbose
    
    - name: Generate High-Quality Training Dataset
      run: |
        echo "üèóÔ∏è TRAINING DATASET GENERATION"
        echo "=============================="
        
        # Use Standard RAG results for training dataset generation
        EVAL_RESULTS="autorag_results/standard_rag/qa_rag_evaluation_report.json"
        echo "üìà Using Standard RAG results for training dataset generation"
        
        python training_dataset_generator.py \
          --evaluation-results "$EVAL_RESULTS" \
          --output-dir autorag_results \
          --min-similarity 0.3 \
          --min-length-ratio 0.5 \
          --max-length-ratio 2.0 \
          --min-answer-length 20 \
          --verbose

    - name: Run Audio Equipment Domain Specificity Evaluation
      run: |
        python domain_eval_gpu.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --results-dir outputs \
          --config audio_equipment_domain_questions.json \
          --max-questions ${{ github.event.inputs.max_questions || '15' }} \
          ${{ github.event.inputs.enable_bert_score == 'false' && '--no-bert-score' || '' }} \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }}

    # Single artifact upload at the end with all results
    - name: Upload Complete Pipeline Results
      uses: actions/upload-artifact@v4
      with:
        name: pdf-qa-standard-rag-complete
        path: |
          outputs/
          rag_input/
          rag_store/
          autorag_results/
          domain_eval_results.csv
          domain_eval_analysis.json
        retention-days: 30
        
    - name: List Available FAISS Index for Download
      run: |
        echo "üì¶ ============================================"
        echo "   AVAILABLE FAISS INDEX IN ARTIFACTS"
        echo "============================================"
        echo "üîç Standard RAG Index:"
        ls -la rag_store/qa_faiss_index.bin 2>/dev/null || echo "   No index found"
        echo ""
        echo "üìÅ Download from GitHub Actions artifacts:"
        echo "   - rag_store/qa_faiss_index.bin (Standard RAG)"
        echo "   - rag_store/qa_metadata.json (Q&A pairs)"
        echo "   - rag_store/model_info.json (Build config)"

    - name: Display Pipeline Summary
      run: |
        echo "üéØ ============================================"
        echo "   STANDARD RAG Pipeline - FINAL RESULTS"
        echo "üöÄ Standard RAG vs Base Model Comparison"
        echo "=============================================="
        
        echo "üì¶ FAISS INDEX CREATED:"
        echo "======================="
        if [ -f "rag_store/qa_faiss_index.bin" ]; then
          echo "  ‚Ä¢ qa_faiss_index.bin (Standard RAG)"
        else
          echo "  ‚ö†Ô∏è No FAISS index found"
        fi
        
        echo ""
        echo "üìä RAG vs BASE MODEL COMPARISON:"
        echo "==============================="
        if [ -f "autorag_results/rag_vs_base_comparison.json" ]; then
          python -c "
        import json
        with open('autorag_results/rag_vs_base_comparison.json', 'r') as f:
            report = json.load(f)
        
        summary = report['summary']
        print(f'Winner: {summary[\"winner\"].upper()}')
        print(f'Key Finding: {summary[\"key_finding\"]}')
        print(f'Recommendation: {summary[\"recommendation_summary\"]}')
        print()
        
        # Show key metrics comparison
        metrics = report['metrics_comparison']
        print('üìà Performance Comparison:')
        print(f'  RAG BERT F1: {metrics[\"standard_rag\"][\"avg_bert_score\"]:.3f}')
        print(f'  Base BERT F1: {metrics[\"base_model\"][\"avg_bert_score\"]:.3f}')
        print(f'  RAG Quality Rate: {metrics[\"standard_rag\"][\"quality_retention_rate\"]:.3f}')
        print(f'  Base Quality Rate: {metrics[\"base_model\"][\"quality_retention_rate\"]:.3f}')
        
        if 'differences' in report['metrics_comparison']:
            differences = report['metrics_comparison']['differences']
            for metric in ['avg_bert_score', 'quality_retention_rate']:
                if metric in differences:
                    diff = differences[metric]
                    improvement = diff['improvement_percent']
                    print(f'  {metric.replace(\"_\", \" \").title()}: {improvement:+.1f}% improvement with RAG')
          "
        else
          echo "‚ö†Ô∏è RAG vs Base comparison report not found"
        fi
        
        echo ""
        echo "üé∏ RAG DEMONSTRATION RESULTS:"
        echo "============================"
        if [ -f "autorag_results/rag_demonstration.json" ]; then
          python -c "
        import json
        with open('autorag_results/rag_demonstration.json', 'r') as f:
            demo = json.load(f)
        
        summary = demo['demonstration_summary']
        print(f'Questions Demonstrated: {summary[\"total_questions\"]}')
        print(f'Average Context Relevance: {summary[\"performance_metrics\"][\"avg_context_relevance\"]:.3f}')
        print(f'Average RAG Generation Time: {summary[\"performance_metrics\"][\"avg_rag_generation_time_ms\"]:.1f}ms')
        print(f'Average Base Generation Time: {summary[\"performance_metrics\"][\"avg_base_generation_time_ms\"]:.1f}ms')
        
        # Show sample question
        if demo['demonstrations']:
            sample = demo['demonstrations'][0]
            print(f'\\nüîç Sample Question: {sample[\"question\"]}')
            print(f'üìö Retrieved {len(sample[\"retrieved_context\"])} relevant context pairs')
          "
        else
          echo "‚ö†Ô∏è RAG demonstration results not found"
        fi
        
        echo ""
        if [ -f "domain_eval_analysis.json" ]; then
          echo "üìà DOMAIN EVALUATION RESULTS:"
          echo "============================"
          python -c "
        import json
        with open('domain_eval_analysis.json', 'r') as f:
            analysis = json.load(f)
        
        print(f'Domain Evaluation Questions: {analysis[\"total_questions\"]}')
        print(f'Base Model Domain Relevance: {analysis[\"avg_base_domain_relevance\"]:.3f}')
        print(f'RAG Model Domain Relevance: {analysis[\"avg_rag_domain_relevance\"]:.3f}')
        
        if 'avg_base_bert_f1' in analysis:
            print(f'Base Model BERT F1: {analysis[\"avg_base_bert_f1\"]:.3f}')
            print(f'RAG Model BERT F1: {analysis[\"avg_rag_bert_f1\"]:.3f}')
          "
        fi
        
        echo ""
        echo "üöÄ READY FOR DEPLOYMENT AND TRAINING:"
        echo "===================================="
        echo "‚úÖ High-quality Q&A dataset: autorag_results/high_quality_pairs.jsonl"
        echo "‚úÖ Standard RAG FAISS index: rag_store/qa_faiss_index.bin"  
        echo "‚úÖ RAG vs Base comparison: autorag_results/rag_vs_base_comparison.json"
        echo "‚úÖ RAG demonstration results: autorag_results/rag_demonstration.json"
        echo "‚úÖ Domain analysis: domain_eval_analysis.json"
        echo ""
        echo "üí° Next Steps:"
        echo "  1. Deploy Standard RAG for production use"
        echo "  2. Use high_quality_pairs.jsonl for model fine-tuning"
        echo "  3. Integrate FAISS index with frontend applications"
        echo "  4. Reference demonstration results for user examples"